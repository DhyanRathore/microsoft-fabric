{"cells":[{"cell_type":"markdown","source":["# **Finding the SharePoint Integration Feature Dependent Power BI Reports in a Tenant**\n","\n","# Check out this [blog post](https://bits2bi.com/2025/10/08/finding-the-hidden-connections-power-bi-reports-born-in-sharepoint/) for detailed context about this notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"506113c5-45a5-479a-a17a-f2e7ab44293f"},{"cell_type":"markdown","source":["##### **Credits**\n","The code used for making Power BI REST API calls, processing, storing JSON results, etc. is adapted from FUAM (Fabric Unified Admin Monitoring). Big thanks to their awesome developers! Check out the FUAM [here](https://github.com/microsoft/fabric-toolbox/tree/main/monitoring/fabric-unified-admin-monitoring).\n","\n","##### **Permissions**\n","The user running this notebook must be a **Fabric administrator**.\n","\n","##### **Instructions**\n","\n","- Attach a Lakehouse to the notebook\n","- Update the variables in the Input Section\n","- Run the notebook\n","\n","**⚠️ Note:** The code assumes that only Power BI workspaces created through the SharePoint integration feature have descriptions starting with \"Sharepoint\".\n","If you have other workspaces with descriptions that also start with \"Sharepoint\", review the results carefully and exclude those from the final output as needed."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d51a36e-34b2-4c98-a2e5-915f90783dd4"},{"cell_type":"markdown","source":["## Prepare the environment"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13d70e96-d518-4ccc-8aab-077b00421ca6"},{"cell_type":"code","source":["# Import necessary libraries\n","from sempy import fabric\n","from notebookutils.mssparkutils.credentials import getToken\n","\n","import pandas as pd\n","import numpy as np\n","import requests\n","import json\n","\n","import datetime # For adding timesteamps\n","import time # For using sleep()\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"12719643-ec8f-49e0-8a8a-7db8ef28fcb5"},{"cell_type":"code","source":["# <--------------------- I N P U T --------------------- S E C T I O N --------------------->\n","\n","# ---- SET THE write_to_files TO FALSE IF YOU DO NOT WANT THE SCANNER API RESULTS TO BE STORED IN THE LAKEHOUSE ---- #\n","write_to_files = True\n","\n","# ---- SET THE write_the_processed_df_as_delta_table TO FALSE IF YOU DO NOT WANT THE PROCESSED DATAFRAME TO BE STORED AS A DELTA TABLE ---- #\n","write_the_processed_df_as_delta_table = True\n","\n","# ---- SET THE delta_table_name FOR YOUR TARGET DELTA TABLE ---- #\n","delta_table_name = \"sharepoint_integration_feature_pbi_reports\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2652bedb-7417-4b42-ad29-0569274d665c"},{"cell_type":"code","source":["## Variables\n","\n","# Scanner API request specific\n","workspaces_per_request = 100\n","max_parallel_requests = 16\n","\n","# Set date helpers\n","current_time = datetime.datetime.now()\n","\n","# Init the REST client\n","client = fabric.FabricRestClient()\n","\n","# Array of scan results\n","results = []"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"60c30445-a52d-4ce0-b912-543bafaeefc6"},{"cell_type":"markdown","source":["## Power BI REST API calls"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"07b7b1c5-839b-49c2-a842-a99901433eab"},{"cell_type":"markdown","source":["### Helper functions for REST API calls"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1c43298b-f57e-4f4e-99fe-e6b3d489e73e"},{"cell_type":"code","source":["def GenerateHeader():\n","    # Retrieve an access token for the logged-in user using mssparkutils\n","    # This token is scoped for Power BI (\"pbi\") and is used for authenticated API calls\n","    # Reference: https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities#get-token\n","    access_token = getToken(\"pbi\")\n","\n","    # Construct the HTTP headers required for API requests\n","    # - 'Content-Type': specifies the format of the request body as JSON\n","    # - 'Authorization': includes the Bearer token for secure access\n","    header = {\n","        'Content-Type': 'application/json',\n","        'Authorization': f'Bearer {access_token}'\n","    }\n","\n","    # Return the prepared headers for use in API calls\n","    return header"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad1ca79c-bf20-4af3-9eb3-ad7b845cd02f"},{"cell_type":"code","source":["def RequestWithRetry(\n","    method, url, data={}, num_retries=3, success_list=[200, 202, 404], **kwargs\n","):\n","    # Generate authorization headers for the request\n","    headers = GenerateHeader()\n","    response = None\n","\n","    # Attempt the request up to `num_retries` times\n","    for i in range(num_retries):\n","        try:\n","            # Make the appropriate HTTP request based on the method\n","            if method == \"post\":\n","                response = client.post(url, json=data, headers=headers, **kwargs)\n","            if method == \"get\":\n","                response = client.get(url, headers=headers, **kwargs)\n","\n","            # If the response status code is in the success list, return the response\n","            if response.status_code in success_list:\n","                return response\n","\n","            # Handle rate limiting (HTTP 429 Too Many Requests)\n","            if response.status_code == 429:\n","                retry_after = response.headers.get(\"Retry-After\", None)\n","\n","                # Case 1: Hit the 500 requests per hour limit\n","                if retry_after is not None:\n","                    waitTime = int(retry_after)\n","                    print(\n","                        f\"Hit the 500 requests per hour rate limit - waiting {waitTime} seconds until next retry\"\n","                    )\n","                    time.sleep(waitTime)\n","\n","                # Case 2: Hit the 16 simultaneous requests limit (no Retry-After header)\n","                else:\n","                    waitTime = 120\n","                    print(\n","                        f\"Hit the 16 simultaneous requests limit - waiting {waitTime} seconds until next retry\"\n","                    )\n","                    time.sleep(waitTime)\n","\n","        except Exception as e:\n","            # Log any unexpected exceptions and continue retrying\n","            print(e)\n","            pass\n","\n","    # Return the last response (could be None or an error response)\n","    return response"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc3f532a-44d4-45ac-a1e4-8a8ab6ba5efb"},{"cell_type":"markdown","source":["### Get the list of workspaces\n","\n","**API Documentation:** https://learn.microsoft.com/en-us/rest/api/power-bi/admin/groups-get-groups-as-admin"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b25da55-4788-4877-8fda-34a8d48dd02f"},{"cell_type":"code","source":["# Send a GET request to the Power BI Admin API to retrieve up to 5000 workspaces\n","# Filter only those whose description starts with 'Sharepoint'\n","\n","response = RequestWithRetry(\n","    \"get\",\n","    \"v1.0/myorg/admin/groups?$top=5000&$filter=startswith(description,'Sharepoint')\",\n",")\n","\n","# Normalize the 'value' field from the JSON response into a flat DataFrame\n","# This makes nested JSON data easier to work with in tabular format\n","filtered_workspaces = pd.json_normalize(response.json()[\"value\"])\n","\n","filtered_workspaces_count = len(filtered_workspaces)\n","\n","# Print the total number of filtered workspaces\n","print(\"Total workspaces:\", filtered_workspaces_count)\n","\n","# Display a count of workspaces grouped by their description\n","# Useful for understanding how many workspaces fall under each SharePoint category\n","print(\"Count by description:\")\n","display(\n","    filtered_workspaces.groupby(\"description\", as_index=False)[\"id\"]\n","    .count()\n","    .rename(columns={\"id\": \"count\"})  # Rename for clarity\n",")\n","\n","# Display the full filtered workspace DataFrame for inspection or further analysis\n","display(filtered_workspaces)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"386a8756-f36d-4f77-91cd-03ed9a521565"},{"cell_type":"markdown","source":["### Batch the list of workspaces for Scanner API calls"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cbec263c-0100-4014-bde2-c46111fcb118"},{"cell_type":"code","source":["# Add a numeric index column to the DataFrame for grouping\n","filtered_workspaces[\"index\"] = pd.to_numeric(filtered_workspaces.index)\n","\n","# Calculate the 'run' group by dividing the index by the number of workspaces per request\n","# This helps in batching workspaces into manageable chunks for Scanner API calls\n","filtered_workspaces[\"run\"] = filtered_workspaces[\"index\"] // workspaces_per_request\n","\n","# Group workspace IDs by 'run' and convert them into lists\n","# Each group (run) will contain a list of workspace IDs to be processed together\n","filtered_workspaces = filtered_workspaces.groupby(\"run\")[\"id\"].apply(list)\n","\n","# Initialize a new DataFrame to track the status of each run\n","df_runs = pd.DataFrame(data=filtered_workspaces)\n","\n","# Add a status column to monitor progress of each run (e.g., API call status)\n","df_runs[\"status\"] = \"Not Started\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0351d04a-3a1f-46e3-8704-2cc23987ee81"},{"cell_type":"markdown","source":["### Scanner APIs to get the artifact level details for the workspaces\n","\n","**API Documentation:**\n","- https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-post-workspace-info\n","- https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-status\n","- https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-result"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35273329-fb0c-4bda-abf6-692ff09f7ab6"},{"cell_type":"code","source":["# Filter runs that are either not started, already requested, or currently running\n","# Limit to a maximum number of parallel requests (e.g., 16)\n","df_runs_current = df_runs[\n","    df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])\n","].head(max_parallel_requests)\n","\n","# Continue processing while there are runs in progress or pending\n","while df_runs_current.shape[0] > 0:\n","    # Wait briefly before checking statuses or sending new requests\n","    time.sleep(5)\n","\n","    # Iterate through each run in the current batch\n","    for i, row in df_runs_current.iterrows():\n","        # If the run hasn't started yet, send a POST request to initiate metadata generation\n","        if row[\"status\"] == \"Not Started\":\n","            payload = {\"workspaces\": row[\"id\"]}\n","            powerBIAPIBaseUri = \"https://api.powerbi.com/v1.0/myorg/\"\n","            api_uri = f\"{powerBIAPIBaseUri}/admin/workspaces/getInfo?lineage=true&datasourceDetails=true\"\n","\n","            # Send the request with retry logic to handle rate limits and transient errors\n","            response = RequestWithRetry(\"post\", api_uri, payload)\n","\n","            # Extract the run ID from the response and update the status\n","            id = pd.json_normalize(response.json())[\"id\"][0]\n","            df_runs.loc[i, \"status\"] = \"Request sent\"\n","            df_runs.loc[i, \"run_id\"] = id\n","\n","        # If the run is already requested or running, check its current status\n","        elif row[\"status\"] in [\"Request sent\", \"Running\"]:\n","            response = RequestWithRetry(\n","                \"get\", f\"/v1.0/myorg/admin/workspaces/scanStatus/{row['run_id']}\"\n","            )\n","\n","            # Extract and update the latest status from the response\n","            stat = pd.json_normalize(response.json())[\"status\"][0]\n","            df_runs.loc[i, \"status\"] = stat\n","\n","    # Refresh the list of runs that are still in progress or pending\n","    df_runs_current = df_runs[\n","        df_runs[\"status\"].isin([\"Not Started\", \"Request sent\", \"Running\"])\n","    ].head(max_parallel_requests)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a101c6e5-bc5e-4d5f-bd1e-73a7a1d7b582"},{"cell_type":"code","source":["print (\"Scan status of each batch:\")\n","df_runs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3e9d511-2e1d-4a92-82a7-cf67e4b5942b"},{"cell_type":"markdown","source":["### Parsing and writing the Scanner API results"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d83e9ae-1afd-426f-b378-77e2bfe18da8"},{"cell_type":"code","source":["# Iterate through each scan run in the tracking DataFrame\n","for i, row in df_runs.iterrows():\n","    # Proceed only if the scan run has completed successfully\n","    if row[\"status\"] == \"Succeeded\":\n","        # Retrieve the scan result using the run_id\n","        response = RequestWithRetry(\n","            \"get\", f\"/v1.0/myorg/admin/workspaces/scanResult/\" + row[\"run_id\"]\n","        )\n","\n","        # Append the JSON result to the results list for further processing or analysis\n","        results.append(response.json())\n","\n","        # If writing to files is enabled, save the result as a JSON file\n","        if write_to_files:\n","            # Construct the folder path using current date and time for organized storage\n","            folder_path = (\n","                mssparkutils.fs.getMountPath('/default')\n","                + \"/Files/SharePoint_Integration_Inventory/\"\n","                + current_time.strftime(\"%Y/%m/%d\")\n","                + \"/\"\n","                + current_time.strftime(\"%H-%M-%S\")\n","                + \"/\"\n","            )\n","\n","            # Create the directory if it doesn't exist\n","            mssparkutils.fs.mkdirs(\"file://\" + folder_path)\n","\n","            # Write the scan result to a JSON file named after the run_id\n","            with open(folder_path + row[\"run_id\"] + \".json\", \"w\") as f:\n","                f.write(json.dumps(response.json()))  # Fixed variable name from `response_json` to `response.json()`\n","                print(\n","                    \"/v1.0/myorg/admin/workspaces/scanResult/\"\n","                    + row[\"run_id\"]\n","                    + \": written to lakehouse\"\n","                )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1770ccc-aac0-4c39-871f-3264c9e6a36c"},{"cell_type":"markdown","source":["## Find the SharePoint site connection details for the workspaces"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9be6526-c36d-4acf-b5d6-0019b7b21d8a"},{"cell_type":"code","source":["# Initialize lists to collect flattened data\n","all_reports = []\n","all_datasets = []\n","all_datasources = []\n","\n","# Iterate through each entry in the results list\n","for entry in results:\n","    # Collect all datasource instances\n","    all_datasources.extend(entry.get('datasourceInstances', []))\n","    \n","    # Iterate through each workspace in the entry\n","    for ws in entry.get('workspaces', []):\n","        ws_id = ws['id']\n","        ws_name = ws['name']\n","        \n","        # Flatten and collect report data\n","        for report in ws.get('reports', []):\n","            report_flat = {\n","                'workspace_id': ws_id,\n","                'workspace_name': ws_name,\n","                **report\n","            }\n","            all_reports.append(report_flat)\n","        \n","        # Flatten and collect dataset data\n","        for dataset in ws.get('datasets', []):\n","            dataset_flat = {\n","                'workspace_id': ws_id,\n","                'workspace_name': ws_name,\n","                **dataset\n","            }\n","            all_datasets.append(dataset_flat)\n","\n","# Convert collected lists to DataFrames\n","reports_df = pd.DataFrame(all_reports)\n","datasets_df = pd.DataFrame(all_datasets)\n","datasource_df = pd.DataFrame(all_datasources)\n","\n","# --- Flatten nested structures ---\n","\n","# Explode the 'datasourceUsages' list so each usage becomes a separate row\n","datasets_exploded = datasets_df.explode('datasourceUsages')\n","\n","# Normalize the JSON structure inside 'datasourceUsages'\n","datasource_details = pd.json_normalize(datasets_exploded['datasourceUsages'])\n","\n","# Combine normalized datasource details with the exploded dataset DataFrame\n","datasets_flattened = pd.concat(\n","    [datasets_exploded.drop(columns=['datasourceUsages']), datasource_details],\n","    axis=1\n",")\n","\n","# Normalize the JSON structure inside 'connectionDetails' from datasource instances\n","connection_details_df = pd.json_normalize(datasource_df['connectionDetails'])\n","\n","# Combine normalized connection details with the original datasource DataFrame\n","datasource_flattened = pd.concat(\n","    [datasource_df.drop(columns=['connectionDetails']), connection_details_df],\n","    axis=1\n",")\n","\n","# --- Join datasets with reports and datasource info ---\n","\n","# Step 1: Join datasets with reports using workspace_id and datasetId\n","reports_datasets_joined = pd.merge(\n","    datasets_flattened[['workspace_id', 'workspace_name', 'id', 'name', 'datasourceInstanceId']],\n","    reports_df[['workspace_id', 'workspace_name', 'id', 'name', 'datasetId']],\n","    left_on=['workspace_id', 'id'],\n","    right_on=['workspace_id', 'datasetId'],\n","    how='left',\n","    suffixes=('_dataset', '_report')\n",")\n","\n","# Step 2: Join the result with datasource details using datasourceInstanceId\n","joined_df = pd.merge(\n","    reports_datasets_joined,\n","    datasource_flattened[['datasourceId', 'datasourceType', 'sharePointSiteUrl']],\n","    left_on='datasourceInstanceId',\n","    right_on='datasourceId',\n","    how='left'\n",")\n","\n","# --- Final cleanup and column renaming ---\n","\n","# Select and rename relevant columns for clarity\n","joined_df = joined_df[[\n","    'workspace_id',\n","    'workspace_name_dataset',\n","    'id_report',\n","    'name_report',\n","    'id_dataset',\n","    'name_dataset',\n","    'datasourceType',\n","    'sharePointSiteUrl'\n","]].rename(columns={\n","    'workspace_name_dataset': 'workspace_name',\n","    'id_report': 'report_id',\n","    'name_report': 'report_name',\n","    'id_dataset': 'semantic_model_id',\n","    'name_dataset': 'semantic_model_name'\n","})\n","\n","# Add a derived column to classify SharePoint site type\n","joined_df['sharePointSiteType'] = np.where(\n","    joined_df['sharePointSiteUrl'].str.contains('personal', case=False, na=False),\n","    'Personal',\n","    'Team'\n",")\n","# Extract SharePoint site owner from the URL if it contains 'personal'\n","joined_df['sharePointSiteOwner'] = joined_df['sharePointSiteUrl'].apply(\n","    lambda url: url.split('/')[-1] if pd.notnull(url) and 'personal' in url.lower() else None\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f355706d-b0be-4b2e-b46d-e215ddbb0d70"},{"cell_type":"markdown","source":["## Write the processed data to delta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"72c0381e-d5ef-4b7d-9f8f-1a126d023db1"},{"cell_type":"code","source":["if write_the_processed_df_as_delta_table:\n","    print(f\"Writing the dataframe to delta table: {delta_table_name}\")\n","\n","    # Convert the Pandas DataFrame to a Spark DataFrame\n","    spark_df = spark.createDataFrame(joined_df)\n","\n","    # Write the Spark DataFrame to a Delta table\n","    # - mode(\"overwrite\"): replaces the existing table if it exists\n","    # - option(\"overwriteSchema\", \"true\"): updates the schema if it has changed\n","    # - format(\"delta\"): uses Delta Lake format for ACID transactions and versioning\n","    # - saveAsTable(): saves the data as a managed table in the metastore\n","    spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(delta_table_name)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e441071-b319-43b8-b8ae-ccbbe76b57ac"},{"cell_type":"markdown","source":["## Conclusions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a26c656-181e-4555-b670-6d7d8bbf352a"},{"cell_type":"code","source":["print(\"--------- SUMMARY ---------\")\n","\n","print(\"Total workspaces:\", filtered_workspaces_count)\n","print(\"Workspaces with Power BI items:\", joined_df[\"workspace_id\"].nunique())\n","print(f\"Workspaces without any Power BI items: {filtered_workspaces_count - joined_df['workspace_id'].nunique()}\")\n","print(\"Total distinct semantic models:\", joined_df[\"semantic_model_id\"].nunique())\n","print(\"Total distinct reports:\", joined_df[\"report_id\"].nunique())\n","\n","print(\"Workspaces by SharePoint Site Type:\")\n","display(\n","    joined_df.groupby(\"sharePointSiteType\", as_index=False)[\"workspace_id\"]\n","    .nunique()\n","    .rename(columns={\"workspace_id\": \"count\"})\n",")\n","\n","print(\"--------- DETAILS ---------\")\n","display(joined_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c65bacea-eebf-4360-8ce4-04d32414d198"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}